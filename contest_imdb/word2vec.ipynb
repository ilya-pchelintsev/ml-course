{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I think they really let the quality of the DVD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm sorry but this is just awful. I have told ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Japenese sense of pacing, editing and musi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the '60's/'70's, David Jason was renowned f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Hail The Woman\" is one of the most moving fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  I think they really let the quality of the DVD...      0\n",
       "1  I'm sorry but this is just awful. I have told ...      0\n",
       "2  The Japenese sense of pacing, editing and musi...      0\n",
       "3  In the '60's/'70's, David Jason was renowned f...      1\n",
       "4  \"Hail The Woman\" is one of the most moving fil...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"word2vec/train.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ilya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ilya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ilya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from gensim.models import Phrases\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "en_stop = list(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if\n",
    "              re.match(r'[^\\W\\d]*$', t) and (len(t) > 2) and (t not in en_stop)]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokens = df['review'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_transformer = Phrases(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set(chain(*bigram_transformer[tokens]))\n",
    "\n",
    "word_count = defaultdict(lambda: 0)\n",
    "word_pos_count = defaultdict(lambda: 0)\n",
    "\n",
    "for sent, label in zip(bigram_transformer[tokens], df.label):\n",
    "    sent = set(sent)\n",
    "    for word in sent:\n",
    "        word_count[word] += 1\n",
    "        word_pos_count[word] += label\n",
    "        \n",
    "word_prob = {word: word_pos_count[word] / word_count[word] for word in word_count}\n",
    "\n",
    "word_entropy = {word: -(1 - p) * np.log(1 - p) - p * np.log(p) for word, p in word_prob.items() if 0 < p < 1}\n",
    "for x in word_prob:\n",
    "    if word_prob[x] == 0 or word_prob[x] == 1:\n",
    "        word_entropy[x] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(bigram_transformer[tokens], sg=1, size=200, window=4, min_count=5, iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 600)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(list_of_tokens):\n",
    "    x = np.array([model.wv[t] * (.7 - word_entropy[t]) for t in list_of_tokens if ((t in model.wv.vocab) and (t in word_entropy))])\n",
    "    if x.size == 0:\n",
    "        return np.zeros((model.wv['film'].size * 1))\n",
    "    return np.hstack((np.mean(x, axis=0), np.min(x, axis=0), np.max(x, axis=0)))\n",
    "\n",
    "    \n",
    "fts = np.array([encode(t) for t in bigram_transformer[tokens]])\n",
    "fts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(fts, df.label.values,\n",
    "                                                    test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  4.3min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=2000, C=50, verbose=1, n_jobs=4).fit(X_train[:, :], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0    0.91435   0.90628   0.91030     16091\n",
      "           1    0.90605   0.91414   0.91008     15909\n",
      "\n",
      "    accuracy                        0.91019     32000\n",
      "   macro avg    0.91020   0.91021   0.91019     32000\n",
      "weighted avg    0.91022   0.91019   0.91019     32000\n",
      "\n",
      "Test\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0    0.91071   0.90040   0.90553      3976\n",
      "           1    0.90268   0.91277   0.90770      4024\n",
      "\n",
      "    accuracy                        0.90663      8000\n",
      "   macro avg    0.90669   0.90659   0.90661      8000\n",
      "weighted avg    0.90667   0.90663   0.90662      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicts = clf.predict(X_train).round()\n",
    "print('Train\\n', classification_report(y_train, predicts, digits=5))\n",
    "\n",
    "predicts = clf.predict(X_test).round()\n",
    "print('Test\\n', classification_report(y_test, predicts, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=2000, C=50, verbose=1, n_jobs=4).fit(fts, df.label.values)\n",
    "\n",
    "test = pd.read_csv('word2vec/test.csv', index_col=0)\n",
    "test_tokens = test['review'].apply(tokenize)\n",
    "test_fts = np.array([encode(t) for t in bigram_transformer[test_tokens]])\n",
    "\n",
    "predicted = clf.predict(test_fts)\n",
    "\n",
    "pd.DataFrame({'Predicted': predicted}).to_csv('solution.csv', index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
